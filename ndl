#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import argparse
import os
parser = argparse.ArgumentParser()
parser.add_argument("--id", help="hentai id")
args = parser.parse_args()


def get_pages(url):
    response = requests.get(url)
    webpage = response.content
    soup = BeautifulSoup(webpage, "html.parser")
    sex = soup.find("span", {"class": "num-pages"})
    return sex.text

def get_image_url(page_url):
    response = requests.get(page_url)
    webpage = response.content
    soup = BeautifulSoup(webpage, "html.parser")
    sex = soup.findAll("img")
    img_url = ""
    for s in sex:
        if s["src"].startswith("https://i.nhentai.net"):
            img_url = s["src"]
    
    
    return img_url



def download_hentai(hentai_id):
    url = f"https://nhentai.net/g/{hentai_id}/1/" 
    number_of_pages = get_pages(url)
    w = f"https://nhentai.net/g/{hentai_id}/"
    if os.path.exists(f"./{hentai_id}"):
        os.chdir(f"./{hentai_id}")
    else:
        os.mkdir(f"./{hentai_id}")
        os.chdir(f"./{hentai_id}")
    

    for i in range(1, int(number_of_pages) + 1):
        file_url = get_image_url(w + str(i))
        p = requests.get(file_url)
        print(f"Downloading {file_url}")
        with open(str(i) + ".jpg", 'wb') as f:
            f.write(p.content)


q = args.id
download_hentai(q)


